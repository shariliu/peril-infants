---
title: "Dangerous ground: One-year-old infants are sensitive to peril in other agents’ action plans"
author: "Shari Liu"
date: "October 25, 2021"
output:
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment = "##", echo=TRUE, warning=FALSE, message=FALSE, cache=FALSE, include=TRUE, dependson="wide")
options(scipen = 0, digits = 2)

## load required packages
ipak <- function (pkg) {
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

packages <- c("multcomp", "lsmeans", "schoRsch", "influence.ME", "lme4", "effects", "lmerTest", "cowplot", "irr", "simr", "wesanderson", "patchwork", "janitor", "sjPlot", "corrplot", "plyr", "tidyverse", "dplyr", "EMAtools")

ipak(packages)
```

```{r prep.data}
detach("package:dplyr", unload = TRUE)
library(dplyr)

# read in data
wide <- read.csv(file = "peril_data_deid.csv", header = TRUE) 
head(wide)
str(wide)

# convert into long format
long <- gather(wide, type, look, testavg_lower:control_deep) 
str(long)

# log transform looks
long$look <- as.numeric(as.character(long$look))
long$loglook <- log(long$look)

# set levels for different kinds of looks
long$type <- factor(long$type)

# subset averaged looks across test pairs (2 observations per participant) and control events
long.avg <- long %>% 
  filter(type == "testavg_higher" | type == "testavg_lower" | type == "control_deep" | type =="control_shallow") %>%
  separate(type, into=c("phase", "type"), sep="_")%>%
  # add age groups (relevant for Experiments 1-3)
  mutate(agegroup = as.factor(case_when(agem < 12 ~ "younger",
                            agem > 12 ~ "older")))

long.avg$type <- factor(long.avg$type)
long.avg$phase <- factor(long.avg$phase)
long.avg$exp <- factor(long.avg$exp)
long.avg$sex <- relevel(as.factor(long.avg$sex), ref = "m")

exp1.avg <-dplyr::filter(long.avg, exp == "Exp.1")
exp2.avg <-dplyr::filter(long.avg, exp == "Exp.2")
exp3.avg <-dplyr::filter(long.avg, exp == "Exp.3")

risk.avg <- long.avg %>%
  filter(cost == "Risk",
         agegroup == "older") %>%
  mutate(task = as.factor(
    case_when(experiment == "RISK13" ~ "infer.value",
              (experiment == "MR13" | experiment == "MR2") ~ "min.risk")))

op <- options(contrasts = c("contr.treatment", "contr.poly")) # treatment contrasts
```

```{r prep.functions}
# function for identifying influential observations, and then returning a new model without them
# INPUTS: model = model name, data = dataset, and subj = column heading for observations
# OUTPUT: model excluding influential subjects
exclude.cooks <- function(model, data, subj) {
  cooks <- cooks.distance(influence(model, subj))
  cutoff <- 4/length(unique(data$subj))
  new.model <- exclude.influence(model, grouping = subj, level=data[which(cooks > cutoff),]$subj)
  return(new.model)
}

# function that computes CIs and returns them in df
gen.ci <- function(model) {
  df <- data.frame(confint(model))
  names(df) <- c("lower", "upper")
  return(df)
}

# function that converts model summary to df
gen.m <- function(model) {
  df <- data.frame(coef(summary(model)))
  names(df) <- c("est", "se", "df", "t", "p")
  return(df)
}

# function that returns column of standardized betas from lmer model
gen.beta <- function(model) {
  f <- data.frame(fixef(model))
  colnames(f) <- "beta"
  return(f)
}
# function that returns age info and number of female infants in a dataset

info <- function(longdata) {
  longdata %>% 
  group_by(subj) %>%
  filter(row_number()==1) %>%
  ungroup() %>%
  summarize(mean = mean(agem), min=range(agem)[1], max=range(agem)[2], f=sum(sex=="f"), n=length(unique(subj)))
}

## Retrieved from : http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/#error-bars-for-within-subjects-variables
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=TRUE,
                      conf.interval=.95, .drop=TRUE) {
  library(plyr)
  
  # New version of length which can handle NA's: if na.rm==T, don't count them
  length2 <- function (x, na.rm=FALSE) {
    if (na.rm) sum(!is.na(x))
    else       length(x)
  }
  
  # This does the summary. For each group's data frame, return a vector with
  # N, mean, and sd
  datac <- ddply(data, groupvars, .drop=.drop,
                 .fun = function(xx, col) {
                   c(N    = length2(xx[[col]], na.rm=na.rm),
                     mean = mean   (xx[[col]], na.rm=na.rm),
                     sd   = sd     (xx[[col]], na.rm=na.rm)
                   )
                 },
                 measurevar
  )
  
  # Rename the "mean" column    
  datac <- plyr::rename(datac, c("mean" = measurevar))
  
  datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean
  
  # Confidence interval multiplier for standard error
  # Calculate t-statistic for confidence interval: 
  # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
  ciMult <- qt(conf.interval/2 + .5, datac$N-1)
  datac$ci <- datac$se * ciMult
  
  return(datac)
}
## Norms the data within specified groups in a data frame; it normalizes each
## subject (identified by idvar) so that they have the same mean, within each group
## specified by betweenvars.
##   data: a data frame.
##   idvar: the name of a column that identifies each subject (or matched subjects)
##   measurevar: the name of a column that contains the variable to be summariezed
##   betweenvars: a vector containing names of columns that are between-subjects variables
##   na.rm: a boolean that indicates whether to ignore NA's
normDataWithin <- function(data=NULL, idvar, measurevar, betweenvars=NULL,
                           na.rm=TRUE, .drop=TRUE) {
  library(plyr)
  
  # Measure var on left, idvar + between vars on right of formula.
  data.subjMean <- ddply(data, c(idvar, betweenvars), .drop=.drop,
                         .fun = function(xx, col, na.rm) {
                           c(subjMean = mean(xx[,col], na.rm=na.rm))
                         },
                         measurevar,
                         na.rm
  )
  
  # Put the subject means with original data
  data <- merge(data, data.subjMean)
  
  # Get the normalized data in a new column
  measureNormedVar <- paste(measurevar, "_norm", sep="")
  data[,measureNormedVar] <- data[,measurevar] - data[,"subjMean"] +
    mean(data[,measurevar], na.rm=na.rm)
  
  # Remove this subject mean column
  data$subjMean <- NULL
  
  return(data)
}

## Summarizes data, handling within-subjects variables by removing inter-subject variability.
## It will still work if there are no within-S variables.
## Gives count, un-normed mean, normed mean (with same between-group mean),
##   standard deviation, standard error of the mean, and confidence interval.
## If there are within-subject variables, calculate adjusted values using method from Morey (2008).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   betweenvars: a vector containing names of columns that are between-subjects variables
##   withinvars: a vector containing names of columns that are within-subjects variables
##   idvar: the name of a column that identifies each subject (or matched subjects)
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySEwithin <- function(data=NULL, measurevar, betweenvars=NULL, withinvars=NULL,
                            idvar=NULL, na.rm=TRUE, conf.interval=.95, .drop=TRUE) {
  
  # Ensure that the betweenvars and withinvars are factors
  factorvars <- vapply(data[, c(betweenvars, withinvars), drop=FALSE],
                       FUN=is.factor, FUN.VALUE=logical(1))
  
  if (!all(factorvars)) {
    nonfactorvars <- names(factorvars)[!factorvars]
    message("Automatically converting the following non-factors to factors: ",
            paste(nonfactorvars, collapse = ", "))
    data[nonfactorvars] <- lapply(data[nonfactorvars], factor)
  }
  
  # Get the means from the un-normed data
  datac <- summarySE(data, measurevar, groupvars=c(betweenvars, withinvars),
                     na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)
  
  # Drop all the unused columns (these will be calculated with normed data)
  datac$sd <- NULL
  datac$se <- NULL
  datac$ci <- NULL
  
  # Norm each subject's data
  ndata <- normDataWithin(data, idvar, measurevar, betweenvars, na.rm, .drop=.drop)
  
  # This is the name of the new column
  measurevar_n <- paste(measurevar, "_norm", sep="")
  
  # Collapse the normed data - now we can treat between and within vars the same
  ndatac <- summarySE(ndata, measurevar_n, groupvars=c(betweenvars, withinvars),
                      na.rm=na.rm, conf.interval=conf.interval, .drop=.drop)
  
  # Apply correction from Morey (2008) to the standard error and confidence interval
  #  Get the product of the number of conditions of within-S variables
  nWithinGroups    <- prod(vapply(ndatac[,withinvars, drop=FALSE], FUN=nlevels,
                                  FUN.VALUE=numeric(1)))
  correctionFactor <- sqrt( nWithinGroups / (nWithinGroups-1) )
  
  # Apply the correction factor
  ndatac$sd <- ndatac$sd * correctionFactor
  ndatac$se <- ndatac$se * correctionFactor
  ndatac$ci <- ndatac$ci * correctionFactor
  
  # Combine the un-normed means with the normed results
  merge(datac, ndatac)
}

# function that returns ICC 
reporticc <- function(output, places) {
  mainstat <- output$value
  upperci <- output$ubound
  lowerci <- output$lbound
  statistic <- paste("ICC=", round(mainstat,places), ", 95% CI [", round(lowerci, places), ", ", round(upperci, places), "]", sep = "")
  return(statistic)
}

# function that returns APA-formatted result from lme4/lmerTest table

# version 1 that reports ci, b, beta, se, p
report <- function(table, index, places, tails, flip) {
  if (tails == "1") {
    p.value <- round(table$p[index], 3)/2 # p values always rounded to 3 places
    howmanytails <- "one-tailed"
  } else {
    p.value <- round(table$p[index], 3) # p values always rounded to 3 places
    howmanytails <- "two-tailed"
  }
  if (p.value < .001) {
    p <- "<.001"
  } else {
    p <- paste("=", str_remove(p.value, "^0+"), sep = "") 
  }
  if (missing(flip)) {
    result <- paste("[", round(table$lower[index], places), ",", round(table$upper[index], places), "], ß=", round(table$beta[index], places), ", B=", round(table$est[index],places), ", SE=", round(table$se[index],places), ", p", p, ", ", howmanytails, sep = "")
  } else {
    result <- paste("[", -round(table$upper[index], places), ",", -round(table$lower[index], places), "], ß=", -round(table$beta[index], places), ", B=", -round(table$est[index],places), ", SE=", round(table$se[index],places), ", p", p, ", ", howmanytails, sep = "")
  }
  return(result)
}

# version 2, more condensed, that reports ci, beta, t(df), p
report2 <- function(table, index, places, tails, flip) {
  if (tails == "1") {
    p.value <- round(table$p[index], 3)/2 # p values always rounded to 3 places
    howmanytails <- "one-tailed"
  } else {
    p.value <- round(table$p[index], 3) # p values always rounded to 3 places
    howmanytails <- "two-tailed"
  }
  if (p.value < .001) {
    p <- "<.001"
  } else {
    p <- paste("=", str_remove(p.value, "^0+"), sep = "") 
  }
  if (missing(flip)) {
    result <- paste("[", round(table$lower[index], places), ",", round(table$upper[index], places), "], ß=", round(table$beta[index], places), ", t(", round(table$df[index],2), ")=", round(table$t[index], places), ", p", p, ", ", howmanytails, sep = "")
  } else {
    result <- paste("[", -round(table$upper[index], places), ",", -round(table$lower[index], places), "], ß=", -round(table$beta[index], places), ", t(", round(table$df[index],2), ")=", -round(table$t[index], places), ", p", p, ", ", howmanytails, sep = "")
  }
  return(result)
}

```

```{r summary}
## get within-subjects CIs for plotting

# warning about Nan has to do with missing observations for control events
summary.avg <- summarySEwithin(data = risk.avg, measurevar = "look", betweenvars = c("exp"), withinvars = c("type", "phase"), idvar = "subj") %>%
  drop_na() %>%  
  mutate(cliff = type)
levels(summary.avg$cliff) <- c("deep", "deep", "shallow", "shallow")
levels(summary.avg$phase) <- c("control", "test")

```

```{r ntrialsexcluded}

# figure out how many looks are missing from the dataframe
nexclude <- wide %>%
   gather(type, look, control_1:test4) %>%
   filter(cost=="Risk") %>%
   mutate(missing = case_when(is.na(look) | str_detect(look, "NA") ~ 1)) %>%
   group_by(exp) %>%
   count(missing) %>%
  filter(!is.na(missing)) %>%
  rename(n_missing = n) %>%
  select(!missing)

experiments <- c("Exp.1", "Exp.2", "Exp.3")
totaltrials.Exp1 <- wide %>%
   gather(type, look, fam1:test4) %>% # no control trials
   filter(cost=="Risk") %>%
   filter(exp == "Exp.1") %>%
   group_by(exp) %>%
   tally() %>%
   rename(total = n)

totaltrials.Exp23 <- wide %>%
   gather(type, look, control_1:test4) %>%
   filter(cost=="Risk") %>%
   filter(exp == "Exp.2" | exp == "Exp.3") %>%
   group_by(exp) %>%
   tally() %>%
   rename(total = n)

ntrials <- full_join(nexclude, rbind(totaltrials.Exp1, totaltrials.Exp23)) %>% na.omit()
```

## Reliability + Distribution Info
```{r reliability}

rel <- read.csv(file = "peril_reliability_deid.csv", header=TRUE)
exp1 <- rel %>% filter(experiment.new=="Exp.1")
exp2 <- rel %>% filter(experiment.new=="Exp.2")
exp3 <- rel %>% filter(experiment.new=="Exp.3")

exp1rel <- icc(data.frame(exp1$secondary.look, exp1$orig.look),model="one", type="agreement")

exp2rel <- icc(data.frame(exp2$secondary.look, exp2$orig.look),model="one", type="agreement")

exp3rel <- icc(data.frame(exp3$secondary.look, exp3$orig.look),model="one", type="agreement")

```

```{r log, include = TRUE}
normal.ll <- fitdistr(na.omit(risk.avg$look), "normal")$loglik
lognormal.ll <- fitdistr(na.omit(risk.avg$look), "lognormal")$loglik
```

# Figures
```{r fig.exp1, include=TRUE, fig.width=4}

theme_set(theme_cowplot(font_size=20))

exp1.fig.data <- risk.avg %>% filter(task == "infer.value",
                                           phase == "testavg")
levels(exp1.fig.data$type) <- c(NA, "higher", "lower", NA)
exp1.fig.data$type <- relevel(exp1.fig.data$type, ref = "higher")
colors1 <- c(wes_palettes$Zissou1[3], wes_palettes$Zissou1[2])

risk1 <- ggplot(data = exp1.fig.data %>% filter(exp=="Exp.1"), aes(type, look, fill = type))+
  geom_boxplot()+
  scale_fill_manual(values=colors1)+
    geom_errorbar(data = summary.avg %>% filter(exp =="Exp.1"), colour="red", position = position_dodge(width = 5), width = 0, aes(ymin=look-ci, ymax=look+ci)) +
  stat_summary(fun.y = mean, alpha = 0.8, geom = "point", shape=21, size=3, position = "dodge", colour = "red", fill = "red") +
  ylab("Looking Time (s)") +
  xlab("Test event") +
  coord_cartesian(ylim = c(0, 65)) +
  geom_point(alpha = 0.1)+
  geom_line(alpha = 0.2, aes(group = subj))+
  theme(legend.position="none")+
  scale_x_discrete(labels = c("higher\nvalue", "lower\nvalue"))
  # annotate("text", colour="red", x=1.5, y=63, size=5, label=c("*ß=0.354", "ß=0.168")) +
  # facet_wrap(~exp, scales = "fixed", drop=TRUE)
  # theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

risk1
```

**Figure 2.** Looking time towards test events in Experiment 2. 

```{r fig.exp2, include=TRUE, fig.width=8, fig.height=8}

theme_set(theme_cowplot(font_size=20))

exp23.figure <- rbind(exp2.avg,exp3.avg) %>%
  filter(agegroup == "older") %>%
  mutate(cliff = case_when(type=="deep" | type =="higher" ~ "deep",
                           type=="shallow" | type =="lower" ~ "shallow"))
exp23.figure$cliff <- as.factor(exp23.figure$cliff)
exp23.figure$cliff <- relevel(exp23.figure$cliff, ref = "shallow")
exp23.figure$phase <- as.factor(exp23.figure$phase)

levels(exp23.figure$cliff)
levels(exp23.figure$phase) <- c("control", "test")
exp23.colors <- c(wes_palette("Royal2")[2], wes_palette("Royal2")[1])

exp2.figure <- ggplot(data = exp23.figure %>% filter(exp == "Exp.2"), aes(cliff, look, fill = cliff)) +
  geom_boxplot(aes(alpha=phase))+
  stat_summary(fun.y = mean, alpha = 0.8, geom = "point", shape=21, size=3, position = "dodge", colour = "red", fill = "red") +
  geom_errorbar(data = summary.avg %>% filter(exp == "Exp.2"), colour="red", position = position_dodge(width = 5), width = 0, aes(ymin=look-ci, ymax=look+ci)) +
  ylab("Looking Time (s)") +
  xlab("Cliff Depth") +
  coord_cartesian(ylim = c(0, 65)) +
  geom_point(alpha = 0.1)+
  geom_line(alpha = 0.2, aes(group = subj))+
  facet_wrap(~phase, nrow=1)+
  theme(legend.position="none")+
  scale_fill_manual(values = exp23.colors)+
  scale_alpha_discrete(range=c(0.4, 1))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

exp3.figure <- ggplot(data = exp23.figure %>% filter(exp == "Exp.3"), aes(cliff, look, fill = cliff)) +
  geom_boxplot(aes(alpha=phase))+
  stat_summary(fun.y = mean, alpha = 0.8, geom = "point", shape=21, size=3, position = "dodge", colour = "red", fill = "red") +
  geom_errorbar(data = summary.avg %>% filter(exp == "Exp.3"), colour="red", position = position_dodge(width = 5), width = 0, aes(ymin=look-ci, ymax=look+ci)) +
  ylab("Looking Time (s)") +
  xlab("Cliff Depth") +
  coord_cartesian(ylim = c(0, 65)) +
  geom_point(alpha = 0.1)+
  geom_line(alpha = 0.2, aes(group = subj))+
  facet_wrap(~phase, nrow=1)+
  theme(legend.position="none")+
  scale_fill_manual(values = exp23.colors)+
  scale_alpha_discrete(range=c(0.4, 1))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))



exp2.figure + exp3.figure + plot_annotation(tag_levels = 'A')

```

**Figure 4.**  Looking times from Experiments 2 (A, in-lab) and 3 (B, online) during the control events (lighter) and the test events (darker).


# Experiment 1: Inferring value from risk

```{r exp1.primary}
exp1.avg$type <- relevel(exp1.avg$type, ref = "higher")


exp1.0 <- lmer(loglook ~ 1 + (1|subj),
               data = exp1.avg)

exp1.1 <- lmer(loglook ~ type + (1|subj),
               data = exp1.avg)

# no influential observations
plot(influence(exp1.1, "subj"), which="cook",
     cutoff=4/32, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")

exp1.1.table <- gen.m(exp1.1)
exp1.1.ci <- gen.ci(exp1.1)[3:4,]

exp1.1.beta <- lmer(scale(loglook) ~ type + (1|subj),
               data = exp1.avg )
exp1.1.betas <- gen.beta(exp1.1.beta)

exp1.results <- cbind(exp1.1.table, exp1.1.betas,exp1.1.ci)


# effect size
exp1.cohensd <- lme.dscore(exp1.1,
           data = exp1.avg %>% filter(subj != "S4_13"),
           type = "lme4") %>% select(d) %>% as.numeric()

```

## Methods

### Participants
 Our final sample of participants included `r info(exp1.avg)$n` thirteen-month-old infants (M=`r info(exp1.avg)$mean` months, range=`r info(exp1.avg)$min`-`r info(exp1.avg)$max`, `r info(exp1.avg)$f` female).  Seven infants were excluded and replaced due to fussiness (3 infants) or inattentiveness during test trials (4 infants). Participants were recruited through a database of families who expressed interest in cognitive development research in the Boston area. Of the families in this database who chose to provide demographic information, 79.5% identified their children as White, 10.2% as Asian, 6.9% as Other, 2.5% as Black or African American, 0.4% as American Indian/Alaska Native, and 0.4% as Native Hawaiian/Pacific Islander; 90.3% as not Hispanic or Latino, 9.5% as Hispanic or Latino, and 0.2% as both. Most families in the database (90.4%) had at least one parent or legal guardian with a college diploma or higher. All data were collected at the Harvard Laboratory for Developmental Studies with procedures approved by the Committee on the Use of Human Subjects. We studied 13-month-old infants, rather than the 10-month-old infants tested in our past research [13]) because the younger infants lack experiences with walking and falling that may foster the development of these abilities. The sample size was chosen based on a simulation power analysis over the confirmatory analyses from 2 previous experiments with similar structure, conducted with 10-month-old infants: Experiments 1-2 from [13]), and we collected data until we attained our pre-specified N. The full pre-registration document, including full details about methods, sample size, hypotheses, and analysis plan, is available at  https://osf.io/bfvdc/files/.


### Data Coding and Analysis Strategy.
Infant looking times were coded online using XHAB (Pinto, 1995), and offline using Datavyu (Datavyu Team, 2014). All experimenters and coders were naive to the order of the test events and unable to see the video events (they relied on sound cues to start each trial). To check for exclusions and coding errors, all test trial data were re-coded in Datavyu and excluded if an infant looked away from a test event without ever having seen the agent jump, or if the trial ended too early or late (`r ntrials$n_missing[1]` out of `r ntrials$total[1]` total familiarization trials). We used these offline coded looking times for our final analyses. To assess the reliability of the data,  (`r ntrials$total[1]/2` out of `r ntrials$total[1]` trials) were re-coded in Datavyu by an additional researcher who was naive to test event order. Reliability was high, `r reporticc(exp1rel,2)`. All decisions to include or exclude trials or participants from our analysis were made by researchers who did not know the order of events shown to that infant.

Infant looking times often are log-normally distributed, including in this dataset (log-likelihood of average looking times during test and control trials for Experiments 1-3 under normal distribution `r normal.ll`, under lognormal distribution = `r lognormal.ll`). Our pre-registered dependent measure therefore was the average looking time towards the higher- or lower-danger choice at test in log seconds.  We report the values of unstandardized B coefficients and 95% confidence intervals in this unit, but our summary statistics and plots feature untransformed looking times for interpretability. We analyzed all looking times using mixed effects models (Bates et al., 2015) implemented in R (R Core Team, 2020). Analyses with repeated measures included a random intercept for participant identity; those conducted over multiple experiments included a random intercept for experiment. For every model, we checked for influential participants using Cook’s Distance (Nieuwenhuis et al., 2012) and excluded participants who exceeded the standard 4/n threshold, where n is the number of participants. The number of participants who met this criterion is listed in every model result; including or excluding them does not change the interpretation of any primary analysis (for results including all observations, see SOM). Data manipulation and plotting were conducted using tidyverse packages (Wickham et al., 2019). Cohen’s D derived from lme models were calculated using the EMAtools package (Kleiman, 2017). To enhance reproducibility, all results were written in R Markdown (Xie et al., 2018). 


## Results

### Pre-registered results.
Infants looked longer when the agent chose the target achieved through the less dangerous action (Mlowervalue=`r summary.avg %>% filter(exp=="Exp.1") %>% filter(type == "lower") %>% select(look) %>% as.numeric()`s, pooled standard error (SE)=`r summary.avg %>% filter(exp=="Exp.1") %>% filter(type == "higher") %>% select(se) %>% as.numeric()`) than when the agent chose the target achieved through the more dangerous action  (Mhighervalue=`r summary.avg %>% filter(exp=="Exp.1") %>% filter(type == "higher") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.1") %>% filter(type == "lower") %>% select(se) %>% as.numeric()` , 95% confidence interval (CI) over difference in log seconds `r report2(exp1.results,2,2,2)`, Cohen's d=`r exp1.cohensd`, no influential participants). As in the experiments of Liu et al. (2017) using closely similar methods, but presenting physically different actions on the two test trials, infants looked longer when this expected outcome did not occur.

# Experiment 2: Minimizing Risk
```{r exp2.primary}

exp2.test <- exp2.avg %>% filter(phase=="testavg") 

exp2.info <- info(exp2.avg)

exp2.0 <- lmer(loglook ~ 1 + (1|subj),
               data = exp2.test)

exp2.1 <- lmer(loglook ~ type + (1|subj),
               data = exp2.test)

# id influential observations
plot(influence(exp2.1, "subj"), which="cook",
     cutoff=4/30, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")
# one influential observation

exp2.1.cooks <- lmer(loglook ~ type + (1|subj),
               data = exp2.test %>% filter(subj != "24-MR"))
exp2.1.table <- gen.m(exp2.1.cooks)
exp2.1.ci <- gen.ci(exp2.1.cooks)[3:4,]

exp2.1.beta <- lmer(scale(loglook) ~ type + (1|subj),
               data = exp2.test %>% filter(subj != "24-MR"))
exp2.1.betas <- gen.beta(exp2.1.beta)

exp2.results <- cbind(exp2.1.table, exp2.1.betas, exp2.1.ci)

# effect size
exp2.cohensd <- lme.dscore(exp2.1.cooks,
           data = exp2.test %>% filter(subj != "24-MR"),
           type = "lme4") %>% select(d) %>% as.numeric() *-1
```

```{r exp2.pre}
exp2.control <- exp2.avg %>% filter(phase=="control") 

exp2.2 <- lmer(loglook ~ type + (1|subj),
               data = exp2.control)

# id influential observations
plot(influence(exp2.2, "subj"), which="cook",
     cutoff=4/30, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")
# two influential observation

exp2.cooks <- lmer(loglook ~ type + (1|subj),
               data = exp2.control %>% filter(subj != "59-MR" & subj != "54-MR"))

exp2.table <- gen.m(exp2.cooks)
exp2.ci <- gen.ci(exp2.cooks)[3:4,]

exp2.beta <- lmer(scale(loglook) ~ type + (1|subj),
               data = exp2.control %>% filter(subj != "59-MR" & subj != "54-MR"))
exp2.betas <- gen.beta(exp2.beta)

exp2.results2 <- cbind(exp2.table, exp2.betas, exp2.ci)

# effect size
exp2.control.cohensd <- lme.dscore(exp2.cooks,
           data = exp2.control %>% filter(subj != "59-MR" & subj != "54-MR"),
           type = "lme4") %>% select(d) %>% as.numeric() *-1
```

```{r exp2.pre.vs.test}
exp2.control.test <- exp2.avg %>%
  mutate(cliff = case_when(type=="deep" | type =="higher" ~ "deep",
                           type=="shallow" | type =="lower" ~ "shallow"))

  
exp2.control.test$type <- as.factor(exp2.control.test$type)

exp2.3 <- lmer(loglook ~ cliff * phase + (1|subj),
                     data = exp2.control.test)

plot(influence(exp2.3, "subj"), which="cook",
     cutoff=4/30, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")
# no influential observations

exp2.3.table <- gen.m(exp2.3)
exp2.3.ci <- gen.ci(exp2.3)[3:6,]

exp2.3.beta <- lmer(scale(loglook) ~ cliff * phase + (1|subj),
                     data = exp2.control.test)
exp2.3.betas <- gen.beta(exp2.3.beta)

exp2.results3 <- cbind(exp2.3.table,exp2.3.betas,exp2.3.ci)

# effect size
exp2.interaction.cohensd <- lme.dscore(exp2.3,
           data = exp2.control.test,
           type = "lme4") %>% slice(3) %>% select(d) %>% as.numeric() *-1
```
This study was originally pre-registered with a sample including both 10-month-old and 13-month-old infants. Because our investigation with 10-month-old infants is still ongoing, we deviate from our pre-registration by reporting only results from the older age group. Data from both age groups are open access at https://osf.io/kz7br/. 

## Methods
### Participants
Our final sample of participants included `r exp2.info$n` thirteen-month-old infants (M=`r exp2.info$mean` months, range=`r exp2.info$min`-`r exp2.info$max`, `r exp2.info$f` female). We chose this sample size using a simulation power analysis over the confirmatory analysis of data from a pilot study, as well as estimates of effect sizes of studies with similar displays and design (S. Liu et al., 2017; S. Liu & Spelke, 2017). Our pre-registration document is available at https://osf.io/efc3g/. We collected data until we attained our pre-specified N. Infants were excluded and replaced in the final sample due to fussiness that prevented study completion (3 infants), inattentiveness during test trials (2 infants), or interference from caregivers (2 infants). 


### Data Coding and Analysis
The data coding and analysis strategies were the same as in Experiment 1. Twenty-five out of `r ntrials$total[2]` total familiarization, control, and test trials were excluded from the analysis based on inattentiveness or coding error. Half the test trials from the experiment (60/120 trials) were re-coded in Datavyu by an additional researcher who was naive to test event order. Reliability was high, `r reporticc(exp2rel,2)`.

## Results
Infants looked longer when the agent, at test, chose to cross the deeper over the shallower trench (Mdeep=`r summary.avg %>% filter(exp=="Exp.2", cliff == "deep", phase=="test") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.2", cliff == "deep", phase=="test") %>% select(se) %>% as.numeric()`; Mshallow=`r summary.avg %>% filter(exp=="Exp.2", cliff == "shallow", phase=="test") %>% select(look) %>% as.numeric()`s,  SE=`r summary.avg %>% filter(exp=="Exp.2", cliff == "shallow", phase=="test") %>% select(se) %>% as.numeric()`; `r report2(exp2.results,2,2,1, flip)`, d=`r exp2.cohensd`, excluding one influential participant).

In contrast, when infants’ attention was drawn to each trench by an attention-getting star that appeared in the path of the agent’s subsequent actions, infants looked longer at events near the shallow trench (Mdeep=`r summary.avg %>% filter(exp=="Exp.2", cliff == "deep", phase=="control") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.2", cliff == "deep", phase=="control") %>% select(se) %>% as.numeric()`;  Mshallow=`r summary.avg %>% filter(exp=="Exp.2", cliff == "shallow", phase=="control") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.2", cliff == "shallow", phase=="control") %>% select(se) %>% as.numeric()`;  `r report2(exp2.results2,2,2,2, flip)`, d=`r exp2.control.cohensd`,excluding 2 influential participants). Looking preferences between the control and test events differed significantly (`r report2(exp2.results3,4,2,2, flip)`, d=`r exp2.interaction.cohensd`, no influential observations). See Figure 4A.

# Experiment 3: Minimizing risk replication (no shattering)

```{r exp3.test}

exp3.control.test <- exp3.avg %>%
  mutate(cliff = case_when(type=="deep" | type =="higher" ~ "deep",
                           type=="shallow" | type =="lower" ~ "shallow"))

exp3.avg.test <- exp3.avg %>% filter(phase=="testavg") 

exp3.info <- info(exp3.avg)

exp3.0 <- lmer(loglook ~ 1 + (1|subj),
               data = exp3.avg.test)

exp3.1 <- lmer(loglook ~ type + (1|subj),
               data = exp3.avg.test)

# id influential observations
plot(influence(exp3.1, "subj"), which="cook",
     cutoff=4/30, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")
# no influential observations

exp3.1.table <- gen.m(exp3.1)
exp3.1.ci <- gen.ci(exp3.1)[3:4,]

exp3.1.beta <- lmer(scale(loglook) ~ type + (1|subj),
               data = exp3.avg.test)
exp3.1.betas <- gen.beta(exp3.1.beta)

exp3.results <- cbind(exp3.1.table, exp3.1.betas, exp3.1.ci)

# effect size
exp3.cohensd <- lme.dscore(exp3.1,
           data = exp3.avg.test,
           type = "lme4") %>% select(d) %>% as.numeric() * 1
```

```{r exp3.pre}
exp3.control <- exp3.control.test %>% filter(phase=="control") 

exp3.2 <- lmer(loglook ~ type + (1|subj),
               data = exp3.control)

# id influential observations
plot(influence(exp3.2, "subj"), which="cook",
     cutoff=4/42, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")
# two influential observation

exp3.2.cooks <- lmer(loglook ~ type + (1|subj),
               data = exp3.control %>% filter(subj != "26" & subj != "28"))

exp3.2.table <- gen.m(exp3.2.cooks)
exp3.2.ci <- gen.ci(exp3.2.cooks)[3:4,]

exp3.2.beta <- lmer(scale(loglook) ~ type + (1|subj),
               data = exp3.control %>% filter(subj != "26" & subj != "28"))
exp3.2.betas <- gen.beta(exp3.2.cooks)

exp3.results2 <- cbind(exp3.2.table, exp3.2.betas, exp3.2.ci)

# effect size
exp3.control.cohensd <- lme.dscore(exp3.2.cooks,
           data = exp3.control %>% filter(subj != "26" & subj != "28"),
           type = "lme4") %>% select(d) %>% as.numeric() *-1
```

```{r exp3.pre.vs.test}

  
exp3.control.test$type <- as.factor(exp3.control.test$type)

exp3.3 <- lmer(loglook ~ cliff * phase + (1|subj),
                     data = exp3.control.test)

plot(influence(exp3.3, "subj"), which="cook",
     cutoff=4/30, sort=TRUE,
     xlab="Cook´s Distance",
     ylab="Subject ID")
# 1 influential observation

exp3.3.cooks <- lmer(loglook ~ cliff * phase + (1|subj),
                     data = exp3.control.test %>% filter(subj != "28"))

exp3.3.table <- gen.m(exp3.3.cooks)
exp3.3.ci <- gen.ci(exp3.3.cooks)[3:6,]

exp3.3.beta <- lmer(scale(loglook) ~ cliff * phase + (1|subj),
                     data = exp3.control.test %>% filter(subj != "28"))
exp3.3.betas <- gen.beta(exp3.3.beta)

exp3.results3 <- cbind(exp3.3.table,exp3.3.betas,exp3.3.ci)

# effect size
exp3.interaction.cohensd <- lme.dscore(exp3.3.cooks,
           data = exp3.control.test %>% filter(subj != "28"),
           type = "lme4") %>% slice(3) %>% select(d) %>% as.numeric() * -1
```

```{r pairwise}
# pairwise0 <- lsmeans(exp3.3.cooks, list(pairwise~cliff|phase)) 
# 
# pairwise.beta <- lsmeans(exp3.3.beta, list(pairwise~cliff|phase)) 
# 
# pairwise.beta.value <- pairwise.beta[[2]] %>% as.data.frame() %>% select(contrast, phase, estimate) %>%
#   rename(beta = estimate)
# 
# pairwise.CI <- confint(pairwise0[[2]]) %>% as.data.frame()
# 
# pairwise.t.p <- pairwise0[[2]] %>% as.data.frame()
# 
# within.exp3.almost <- full_join(pairwise.CI, pairwise.t.p) %>%
#   rename(est = estimate,
#          se = SE,
#          lower = lower.CL,
#          upper = upper.CL,
#          t = t.ratio,
#          p = p.value)
# 
# within.exp3 <- full_join(pairwise.beta.value, within.exp3.almost)
```

```{r other.info.exp3}
exp3.devices<- wide %>%
  filter(exp == "Exp.3") %>%
  tabyl(device)

exp3.highchair <- wide %>%
  filter(exp == "Exp.3") %>%
  tabyl(highchair)

exp3.qualratings <- wide %>%
  filter(exp == "Exp.3") %>%
  summarise(vquality = mean(video_quality, na.rm=TRUE),
            vquality.sd = sd(video_quality, na.rm=TRUE),
            aquality = mean(audio_quality, na.rm=TRUE),
            aquality.sd = sd(audio_quality, na.rm=TRUE))

```

## Methods
### Participants
Our final sample included `r exp3.info$n` twelve- to fifteen-month-old infants (M=`r exp3.info$mean` months, range=`r exp3.info$min`-`r exp3.info$max`, `r exp3.info$f` female): a widened age range that enabled more rapid testing of participants, who were recruited both from our lab database, also through a cross-institution platform for recruitment for developmental cognitive science  (https://childrenhelpingscience.com/). Our preregistered target sample size of 40 was determined based on a simulation power analysis over infants’ looking preferences towards the test events from Experiment 2; our stopping rule was to stop recruiting as soon as we reached our target N, but to finish collecting data if we over-recruited. Thus, our final sample was N=42. A further 6 infants were excluded from the study (3 due to technical issues, 2 due to inattentiveness and 1 due to interference from the caregiver).  Our pre-registration document is available at https://osf.io/96qsf/.

### Procedure
Whereas Experiments 1 and 2 were conducted in a quiet, dark room in a lab setting, Experiment 3 was conducted over Zoom video conferencing, in infants’ homes, due to the COVID-19 pandemic, following procedures approved by the Committee on the Use of Human Subjects at Harvard University. We used materials developed by the Stanford Social Learning Lab (Social Learning Lab, 2020) to introduce caregivers to the online testing setup and to ask for verbal consent. Caregivers also provided written consent prior to the study session. Infants sat in a high chair (`r exp3.highchair$n[2]` out of 42 participants) or their caregivers’ laps (`r exp3.highchair$n[1]`/42), depending on caregiver preferences, and watched the displays on a tablet (`r exp3.devices$n[1]`/42) or a laptop computer (`r exp3.devices$n[2]`/42). We asked caregivers, both before and during the study, to minimize distractions (pets, people walking by, and distracting objects) during the study session. 

Before the experiment, infants saw a calibration video where their attention was drawn to the four corners of the screen, as well as the center of the screen. To maximize the quality of the events seen by infants, we shared our stimuli with caregivers through YouTube playlists, controlled the caregiver’s screen using Zoom’s remote control feature, and coded infants’ looking times during the study using jHab (Casstevens, 2007). Caregivers rated the quality of the audio and video on a 5-point Likert scale (1 = very poor; 5 = very good), giving high ratings, on average, for both (video: M=`r exp3.qualratings$vquality`, SD=`r exp3.qualratings$vquality.sd`; audio: M=`r exp3.qualratings$aquality`, SD=`r exp3.qualratings$aquality.sd`). After the session, we double checked for trial exclusions and generated the final data from the recording of the session video using Datavyu (Datavyu Team, 2014). As before, experimenters only had access to the video feed of infants’ faces (and not the displays) during the experiment, and therefore were unaware of the order of test events. To allow caregivers to attend to safety issues at home, we did not ask them to close their eyes, and instead instructed them to refrain from directing their infants’ attention toward or away from the screen. Our full online testing protocol is described in the SOM. 


#### Data Coding and Analysis
The data coding and analysis strategy was identical to Experiment 2. Fifty-three out of `r ntrials$total[4]`  total trials (including familiarization, test, and control trials) were excluded from analysis because of inattentiveness, distractions at home (e.g. pet noises, people walking by), technical issues and coding errors. The proportion of excluded trials (`r ntrials$n_missing[3]/ntrials$total[3]*100`%) was higher than what we observed in the lab in Experiment 2 (`r ntrials$n_missing[2]/ntrials$total[2]*100`%), due to distractions in the home environment, the smaller size of the screen displaying the videos at home, and the lower or more variable quality of the video feeds of the infants’ faces (which led to trial mis-timings). As in Experiments 1-2, 50% of the test trials were recoded by an additional naive coder (84 of 168 test trials). Interrater reliability was high, `r reporticc(exp3rel,2)`.

## Results
### Pre-registered results
We fully replicated the two key results from Experiment 2. Infants looked longer at test when the agent chose to jump the deeper trench (Mdeep=`r summary.avg %>% filter(exp=="Exp.3", cliff == "deep", phase=="test") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.3", cliff == "deep", phase=="test") %>% select(se) %>% as.numeric()`; Mshallow=`r summary.avg %>% filter(exp=="Exp.3", cliff == "shallow", phase=="test") %>% select(look) %>% as.numeric()`s,  SE=`r summary.avg %>% filter(exp=="Exp.3", cliff == "shallow", phase=="test") %>% select(se) %>% as.numeric()`; `r report2(exp3.results,2,2,1, flip)`, d=`r exp3.cohensd`, no influential participants). Infants’ looking preferences between the control events and the test events significantly differed from each other (`r report2(exp3.results3, 4, 2, 2, flip)`, d=`r exp3.interaction.cohensd`, excluding 1 influential participant). 

### Exploratory results
During the control events, infants showed a numerical but non-significant preference for the event in which the inanimate object appeared over the shallower trench (Mdeep=`r summary.avg %>% filter(exp=="Exp.3", cliff == "deep", phase=="control") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.3", cliff == "deep", phase=="control") %>% select(se) %>% as.numeric()`; Mshallow=`r summary.avg %>% filter(exp=="Exp.3", cliff == "shallow", phase=="control") %>% select(look) %>% as.numeric()`s, SE=`r summary.avg %>% filter(exp=="Exp.3", cliff == "shallow", phase=="control") %>% select(se) %>% as.numeric()`; `r report2(exp3.results2, 2, 2, 2, flip)`, d=`r exp3.control.cohensd`, excluding 2 influential participants). See Figure 4A.

# Supplemental Online Materials

```{r logplot}
# is a lognormal transformation justified given the distribution of looks?
fig.S1 <- ggplot(data = risk.avg, aes(look, fill=exp))
fig.S1 +
  geom_density(alpha = 0.5)+
  # geom_text(aes(experiment))+
  theme_cowplot(20)+
  # facet_wrap(~exp)+
  xlab("Looking Time (s)")
  # scale_fill_brewer(palette="Set2")
```

**Figure S1.** Density plot of looking times during test for Experiment 1 and from test events and control events for Experiments 2-3. Maximum-likelihood fitting revealed that the lognormal distribution (log likelihood=`r lognormal.ll`) provides a better fit to these data than the normal distribution (log likelihood=`r normal.ll`).

```{r famplot}
fam <- wide %>%
  filter(cost=="Risk",
         (exp == "Exp.1" | exp ==  "Exp.2" | exp== "Exp.3"))%>%
  gather(trial, look, fam1:test4) %>%
  mutate(trial_n = parse_number(trial)) %>%
  mutate(trial_type = str_extract(trial, "[a-z]+"))

fam$trial_type <- as.factor(fam$trial_type)
fam$trial_n <- as.factor(fam$trial_n)
fam$look <- as.numeric(as.character(fam$look))

famplot <- ggplot(data = fam, aes(trial_n, look, fill=trial_type))
famplot + geom_boxplot() +
  facet_wrap(~exp+trial_type, nrow=2)+
  xlab("Trial N")+
  stat_summary(fun.data =mean_cl_boot, geom="errorbar",width=0.1)+
    stat_summary(fun.y=mean,geom="point",shape=5)+
  ylab("Looking time (s)")+
  theme_cowplot(20)
```

**Figure S2.** Boxplots of looking times during familiarization and test across Experiments (total N=`r length(unique(risk.avg$subj))`). Error bars represent bootstrapped 95% confidence intervals around the mean.

## Including influential observations (reviewer suggestion)

Below, we report the results from our pre-registered analyses including all observations, rather than excluding influential observations. We report the analysis for Exp 2 and 3 only because no influential observations were observed in the confirmatory analysis of Experiment 1.  

```{r includingall.exp2}
exp2.everyone.test.table <- gen.m(exp2.1)

exp2.everyone.test.ci <- gen.ci(exp2.1)[3:4,]

exp2.everyone.test.beta <- lmer(scale(loglook)~ type + (1 | subj),
                                     data=exp2.test)

exp2.everyone.test.betas <- gen.beta(exp2.everyone.test.beta)

exp2.everyone.test.results <- cbind(exp2.everyone.test.betas,exp2.everyone.test.table,exp2.everyone.test.ci)

exp2.everyone.pre.table <- gen.m(exp2.2)
exp2.everyone.pre.ci <- gen.ci(exp2.2)[3:4,]

exp2.everyone.pre.beta <- lmer(scale(loglook) ~ type + (1 | subj),
                                     data=exp2.control)
exp2.everyone.pre.betas <- gen.beta(exp2.everyone.pre.beta)

exp2.everyone.pre.results <- cbind(exp2.everyone.pre.betas,exp2.everyone.pre.table,exp2.everyone.pre.ci)

exp2.everyone.prevstest.table <- gen.m(exp2.3)
exp2.everyone.prevstest.ci <- gen.ci(exp2.3)[3:6,]

exp2.everyone.prevstest.beta <- lmer(scale(loglook) ~ cliff * phase + (1|subj), data = exp2.control.test)

exp2.everyone.prevstest.betas <- gen.beta(exp2.everyone.prevstest.beta)

exp2.everyone.prevstest.results <- cbind(exp2.everyone.prevstest.betas,exp2.everyone.prevstest.table,exp2.everyone.prevstest.ci)
```

```{r includingall.exp3}
exp3.everyone.test.table <- gen.m(exp3.1)
exp3.everyone.test.ci <- gen.ci(exp3.1)[3:4,]

exp3.everyone.test.beta <- lmer(scale(loglook)~ type + (1 | subj),
                                     data=exp3.avg)
exp3.everyone.test.betas <- gen.beta(exp3.everyone.test.beta)

exp3.everyone.test.results <- cbind(exp3.everyone.test.betas,exp3.everyone.test.table,exp3.everyone.test.ci)

exp3.everyone.pre.table <- gen.m(exp3.2)
exp3.everyone.pre.ci <- gen.ci(exp3.2)[3:4,]

exp3.everyone.pre.beta <- lmer(scale(loglook) ~ type + (1 | subj),
                                     data=exp3.control)
exp3.everyone.pre.betas <- gen.beta(exp3.everyone.pre.beta)

exp3.everyone.pre.results <- cbind(exp3.everyone.pre.betas,exp3.everyone.pre.table,exp3.everyone.pre.ci)

exp3.everyone.prevstest.table <- gen.m(exp3.3)
exp3.everyone.prevstest.ci <- gen.ci(exp3.3)[3:6,]

exp3.everyone.prevstest.beta <- lmer(scale(loglook) ~ cliff * phase + (1|subj), data = exp3.control.test)

exp3.everyone.prevstest.betas <- gen.beta(exp3.everyone.prevstest.beta)

exp3.everyone.prevstest.results <- cbind(exp3.everyone.prevstest.betas,exp3.everyone.prevstest.table,exp3.everyone.prevstest.ci)
```


### Experiment 2
Infants looked longer at test when the agent, at test, chose the deeper trench over the shallower trench (`r report2(exp2.everyone.test.results, 2, 2, 1, flip)`). During control events, 13-month-old infants preferred to look at the shallow trench (`r report2(exp2.everyone.pre.results,2,2,2,flip)`). Their looking preferences significantly differed across the two phases of the experiment, `r report2(exp2.everyone.prevstest.results,4,2,2, flip)`).  These findings accord with those reported in the main text and support the interpretation that infants expected the agent to take the less dangerous action and therefore showed a greater looking preference for the test event than for the control event presenting events over the deeper trench. 

### Experiment 3
 Infants looked longer at test when the agent chose to jump over the deeper trench (`r report2(exp3.everyone.test.results, 2, 2, 1, flip)`). During control events, infants did not show a looking preference for either event (`r report2(exp3.everyone.pre.results,2,2,2, flip)`). Their looking preferences significantly differed across the test and control trials (`r report2(exp3.everyone.prevstest.results,4,2,2, flip)`).  This finding fully replicates the two key findings from Experiment 2 and accords with the findings reported in the main text.

## Order effects in Experiment 1 (reviewer-requested exploratory analysis)
```{r order.exp1}

exp1.order <- lmer(data = exp1.avg,
                   formula = loglook ~ first_fam * type + (1|subj))
exp1.order.table <- gen.m(exp1.order)
exp1.order.ci <- gen.ci(exp1.order)[3:6,]


exp1.order.betas <- lmer(data = exp1.avg,
                   formula = scale(loglook) ~ first_fam * type + (1|subj))
exp1.order.beta <- gen.beta(exp1.order.betas)

exp1.ordereffects <- cbind(exp1.order.table, exp1.order.beta, exp1.order.ci)

```

Infants’ looking preferences at test did not vary depending on which sequence of events (low to high danger vs high to low danger) they were randomly assigned to watch in the first familiarization trial (`r report2(exp1.ordereffects,4,2,2)`). All infants saw both trial orders for 3 familiarization trials each.

## Looking time to each familiarization event in Experiment 1 (reviewer-requested exploratory analysis)

```{r}
detach("package:dplyr", unload = TRUE)
library(dplyr)
exp1.fam <- read.csv("./exp1_fam_csvs/exp1_fam_looks.csv", header=TRUE)

exp1.fam <- exp1.fam %>%
  separate(videoclip, into = c("depth", "yesno"), remove=FALSE) %>%
  rename(subj= subjID)

exp1.fam$depth <- as.factor(exp1.fam$depth)
exp1.fam$yesno <- as.factor(exp1.fam$yesno)
exp1.fam$trial <- as.factor(exp1.fam$trial)
exp1.fam$subj <- as.factor(exp1.fam$subj)
exp1.fam$videoclip <- as.factor(exp1.fam$videoclip)

exp1.fam.glancedoff <- exp1.fam %>%
  mutate(glanced.off = case_when(proportion.on == 1.0 ~ 0,
                                proportion.on < 1.0 ~ 1))

exp1.fam.glancedoff.totalclips <- exp1.fam.glancedoff %>%
  select(subj, depth, videoclip, glanced.off) %>%
  group_by(subj, videoclip) %>%
  summarise(totalclips = n()) 

exp1.fam.glancedoff.freq <- exp1.fam.glancedoff %>%
  select(subj, depth, videoclip, glanced.off) %>%
  group_by(subj, videoclip) %>%
  tally(glanced.off)

exp1.fam.glanced.off.summary <- full_join(exp1.fam.glancedoff.totalclips, exp1.fam.glancedoff.freq) %>%
  mutate(prop.glancedoff = n/totalclips) %>%
  mutate(depth = case_when(videoclip == "deep_no" ~ "deep",
                           videoclip == "shallow_yes" ~ "shallow",
                           videoclip == "medium_no" ~ "medium",
                           videoclip == "medium_yes" ~ "medium"))

exp1.fam.glanced.off.summary$videoclip <- factor(exp1.fam.glanced.off.summary$videoclip, levels=c("shallow_yes", "medium_no", "medium_yes", "deep_no"))
```

```{r}

figS3 <- exp1.fam.glanced.off.summary %>%
  ggplot(aes(videoclip, prop.glancedoff, fill=depth)) + 
  geom_boxplot() +
  geom_point(alpha=0.3) +
  geom_line(alpha = 0.2, aes(group = subj)) +
  ylab("Proportion of events including look away") +
  xlab("Event type") +
  # facet_wrap(~depth) + 
  stat_summary(fun.data =mean_cl_boot, geom="errorbar",width=0.2)+
    stat_summary(fun=mean,geom="point",shape=5, size=3)+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

figS3
```
**Figure S3.** Proportion of events during which infants glanced away from the screen, relative to how many times infants saw each event. Data come from a random subset of infants in Experiment 1 (N=16 out of 32 total infants), with observations grouped by infant (points connected by grey lines). Error bars represent bootstrapped 95% confidence intervals around the mean. Infants look away from the screen with roughly equal probabilities across the 4 event types.


```{r}
# compute 4 values per subject, total proportion to
# deep_no, medium_yes, medium_no, shallow_yes
exp1.fam.bymovie <- exp1.fam %>%
  group_by(subj,videoclip, depth, yesno) %>%
  mutate(proportion.on.total = mean(proportion.on)) %>%
  distinct(proportion.on.total)

exp1.fam.bymovie.wide <- exp1.fam.bymovie %>%
  pivot_wider(names_from = videoclip, values_from = proportion.on.total, id=subj)

exp1.avg.diff <- exp1.avg %>%
  filter(phase == "testavg") %>%
  pivot_wider(names_from = type, values_from = look, id=subj) %>%
  mutate(delta.look = lower-higher)

exp1.fam.glancedoff.wide <- exp1.fam.glanced.off.summary %>%
  pivot_wider(names_from = videoclip, values_from = prop.glancedoff, id=subj)

exp1.famtest <- full_join(exp1.fam.bymovie.wide, exp1.avg.diff, by=c("subj")) %>% 
  na.omit()
exp1.famtest.long <- exp1.famtest %>%
  gather(key = "movie_clip", value = "proportion_looking", shallow_yes:deep_no)

exp1.famtest.glanceoff <- full_join(exp1.fam.glancedoff.wide, exp1.avg.diff, by=c("subj")) %>% 
  na.omit()

exp1.famtest.glanceofflong <- exp1.famtest.glanceoff %>%
  gather(key = "movie_clip", value = "proportion_glanced_off", shallow_yes:deep_no)

```

```{r}
theme_set(theme_cowplot(font_size=15))

figS4A <- 
exp1.famtest.long %>%
  ggplot(aes(proportion_looking, delta.look)) + 
  geom_point() +
  geom_smooth(method="lm") +
  # geom_line(alpha = 0.2, aes(group = subjID)) +
  xlab("Total proportion looking \n to movie clip") +
  ylab("Looking preference at test (s)\n
       <--- Longer looking to expected ---- Longer looking to unexpected --->") +
  facet_wrap(~movie_clip)

figS4A
```

*Figure Sx [not in final SOM]* Scatter plot of average proportion looking to each movie clip from familiarization and looking preferences at test. 


```{r}
cor.data <- exp1.famtest.glanceoff %>%
  select(shallow_yes:deep_no, delta.look) %>%
  rename(VOE_response = delta.look) %>%
  as.data.frame() 


corrplot(cor(cor.data[,-1]),
         method='circle',
         type='lower',
         addCoef.col ='black',
         diag=FALSE)
```

*Figure S4.* Correlation plot relating infants' likelihood of looking away from each of the 4 familiarization events (proportion of events including a look away) to one another, and to infants' violation of expectation response (unexpected - expected) at test. Values indicate Pearson's correlations. Descriptively, the more infants looked away from the events, the smaller VOE response they showed at test.


```{r}
fam.glance1 <- lmer(data = exp1.fam.glanced.off.summary,
               formula = prop.glancedoff ~ videoclip + (1|subj))
summary(fam.glance1)
tab_model(fam.glance1, show.stat=TRUE,show.df=TRUE)
plot(allEffects(fam.glance1))
```

```{r}

fam.glance2 <- lm(data = exp1.famtest.glanceoff,
             formula = scale(delta.look) ~ scale(shallow_yes) + scale(medium_no) + scale(medium_yes) + scale(deep_no))
tab_model(fam.glance2, show.stat=TRUE,show.df=TRUE)
summary(fam.glance2)
plot(allEffects(fam.glance2))
```

In Experiment 1, infants, on average, looked longer when the agent jumped deeper trenches for one goal than another, and then chose the other goal later at test. One question is how infants used the information in each of the 4 familiarization events, presented in a looping sequence over 6 familiarization trials, in order to draw this inference. Rather than comparing the relative acceptances and refusals of the agent across 3 different levels of peril (shallow, medium, and deep trenches), one alternative hypothesis is that infants selectively attended when the agent accepted and refused the same obstacle (medium trench) for the two goals, and used this ‘go-no-go’ heuristic to infer that the agent prefers the goal it jumped for, over the goal it refused to jump for. 

On this hypothesis, infants should be less likely to glance away from events involving medium trenches (vs the other events), and those who looked away less (i.e. attended more) to the medium trench events should have exhibited larger violation-of-expectation effects at test. To test these predictions, naive coders chose a random 50% of videos from Experiment 1 and annotated the onset and offset times of each iteration of each event in each familiarization loop, ignoring interleaved blank screens, and then annotated the onset and offset of infants’ attention to each event iteration. In the plots and following analyses, the events are named shallow_yes and medium_yes when the agent willingly jumped a shallow or medium trench, and medium_no and deep_no when the agent refused to jump a medium or deep trench.  For each infant, we calculated the number of each kind of event they saw. Then, we calculated the proportion of those events that infants looked away from. If an infant looked away from the screen for any portion of the event, we marked that event as one where they looked away. Otherwise, we marked that event as one where they looked For example, if an infant saw 5 deep_no events and glanced away from the screen for 1 of them, this produced a score of 0.2 for that event type, for that infant. We then averaged these proportions within infants across all 4 event types, to produce 4 different proportion glance-away scores per infant. These scores are plotted in Figure S3, are related to each other, and to infants’ looking preferences at test, in Figure S4.

Overall, infants were equally likely to glance away from the screen (vs attend for the entire duration) during the 4 events. See Table S1 for results of the linear mixed effects model (lmer formula: prop.glancedoff ~ videoclip + (1|subj)). Thus, infants did not attend selectively to the events  where they had the opportunity to compare the agent’s acceptance and refusal of the medium trench towards the two goals. Instead, they were equally likely to glance away from all 4 types of events.

**Table S1**. Infants’ probability of glancing away from the 4 video clips from familiarization in Experiment 1
```{r}

fam1 <- lmer(data = exp1.fam.bymovie,
               formula = proportion.on.total ~ videoclip + (1|subj))
summary(fam1)
tab_model(fam1, show.stat=TRUE, show.df=TRUE)
plot(allEffects(fam1))
```

We then tested the second prediction: that infants who glanced away from the medium trench events (i.e. those who missed the critical information for a go-no-go strategy) would also show smaller violation-of-expectation responses at test. To do this, we calculated infants’ looking preference at test (average duration looking when the agent chose the less valued goal, minus average duration looking when the agent chose the more valued goal), and asked whether variability in infants’ looking behavior towards each of the 4 events predicted variability in these looking preferences. We found that infants’ tendency to glance away from the events involving medium trenches, or towards any of the 4 events, did not predict the magnitude of their violation-of-expectation response. See Table S2 for full results (lm formula: delta.look ~ shallow_yes + medium_no + medium_yes + deep_no). 

Together, these findings suggest that infants did not selectively attend to the videos with the same trench depth during familiarization in Experiment 1 (or selectively glance away from the other events), and that their looking towards these videos did not predict stronger inferences about which goal was more valuable. Therefore, it appears unlikely that infants as a group used a “go-no-go” heuristic on the agent’s actions over the medium trenches in order to infer which the agent preferred. To be clear, we are not suggesting that infants could never use such a strategy. Instead we are suggesting that this strategy does not appear to explain the results of Experiment 1 (based on this analysis), or the results of Experiments 2-3 (in principle, based on the experimental design, in which the agent always accepts and never refuses jumping actions)


**Table S2**. Infants’ violation of expectation responses at test, as predicted by their tendency to glance away from the 4 video clips from familiarization in Experiment 1. Dependent and independent variables were z-scored prior to entry into the model.
```{r}

fam2 <- lm(data = exp1.famtest,
             formula = scale(delta.look) ~ scale(shallow_yes) + scale(medium_no) + scale(medium_yes) + scale(deep_no))
tab_model(fam2, show.stat=TRUE,show.df=TRUE)
summary(fam2)
plot(allEffects(fam2))
```


<!-- ## Power analysis over experiment 1 -->
```{r exp1.power, eval=FALSE}
# sim <- powerCurve(extend(exp1.1, along="subj", n=500),
#                        along="subj", breaks = c(36, 40, 44, 48, 52, 56, 60, 64, 68), alpha = .05, seed = 123)
# plot(sim)
# print(sim)

```


<!-- ## Generate Data for Reliability -->
```{r generate.rel, eval=FALSE}
# reliability <- wide %>% filter(reliability ==1) %>%
#   select(subj, sex, experiment, test1, test2, test3, test4) %>%
#   gather(trial, look, test1:test4) %>%
#   mutate(trialn = str_remove(trial, "test")) %>%
#   group_by(subj, trialn)
# write.csv(reliability, "risk_rel.csv")
```



